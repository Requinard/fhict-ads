{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who's Text is This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a simple classifier to predict the author of a text (string) containing words in a restricted language. An accompanying Excel file exists to generate instances of texts, labeled by an author. The generator instantiates preboiled instances in that the probability for using a word by a particular author is taken into account. As such the generator is at the same time a corpus generator and a means to check if the calculated classifier makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the text corpus and create a sentence list and its label list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Gerrit', 'soccer', 'soccer', 'car', 'difficult', 'difficult', 'soccer', 'difficult', 'soccer', 'car', 'difficult', 'car', '', '', '', ''], ['Gerrit', 'soccer', 'boring', 'car', 'boring', 'contract', 'boring', 'difficult', 'soccer', 'car', 'soccer', 'contract', 'boring', 'car', 'energy', 'soccer']]\n",
      "\n",
      "['soccer soccer car difficult difficult soccer difficult soccer car difficult car', 'soccer boring car boring contract boring difficult soccer car soccer contract boring car energy soccer']\n",
      "['Gerrit', 'Gerrit']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "f = open(\"ML06_Corpus.csv\")\n",
    "corpus = [ row for row in csv.reader(f, delimiter=';')]\n",
    "X = [\" \".join(lst[1:]).strip(\" \") for lst in corpus]\n",
    "y = [lst[0] for lst in corpus]\n",
    "print(corpus[:2])\n",
    " \n",
    "print()\n",
    "\n",
    "print(X[:2])\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, you don't know anything about machine learning. Waht would be an approach to determine the author of a sentence, having this set of labeled sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gerrit': 0.09615384615384616, 'soccer': 0.2153846153846154, 'car': 0.19615384615384615, 'difficult': 0.1076923076923077, 'boring': 0.06923076923076923, 'contract': 0.13076923076923078, 'energy': 0.028846153846153848, 'sweet': 0.15576923076923077}\n",
      "{'Truus': 0.09157509157509157, 'boring': 0.2893772893772894, 'energy': 0.25457875457875456, 'soccer': 0.07326007326007326, 'car': 0.11538461538461539, 'contract': 0.08058608058608059, 'sweet': 0.05311355311355311, 'difficult': 0.04212454212454213}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words_G = [word for row in corpus[:50] for word in row if word != '']\n",
    "words_T = [word for row in corpus[50:] for word in row if word != '']\n",
    "\n",
    "counts_G = Counter(words_G)\n",
    "counts_T = Counter(words_T)\n",
    "\n",
    "total_G = sum(counts_G.values())\n",
    "total_T = sum(counts_T.values())\n",
    "\n",
    "perc_G = {key: value/total_G for  (key, value) in counts_G.items()}\n",
    "perc_T = {key: value/total_T for  (key, value) in counts_T.items()}\n",
    "\n",
    "print(perc_G)\n",
    "print(perc_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True;False;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;False;True;True;True;True;True;True;False;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;True;\n",
      "=====\n",
      "False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;True;False;False;False;True;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;False;"
     ]
    }
   ],
   "source": [
    "def determine_author(sentence, perc1, perc2):\n",
    "    prod1 = prod2 = 1.0\n",
    "    for word in sentence:\n",
    "        prod1 *= perc1[word]\n",
    "        prod2 *= perc2[word]\n",
    "        # print(prod1, prod2)\n",
    "    return prod1 > prod2\n",
    "\n",
    "# determine_author(X[1].split(\" \"), perc_G, perc_T)\n",
    "for sentence in X[:50]:\n",
    "    print(determine_author(sentence.split(\" \"), perc_G, perc_T), end=';')\n",
    "\n",
    "print(\"\\n=====\")\n",
    "\n",
    "for sentence in X[50:]:\n",
    "    print(determine_author(sentence.split(\" \"), perc_G, perc_T), end=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's interesting and perhaps a bit embarassing at the same time. I would have expected the first set of 50 all True (author = 'Gerrit') and the subsequent 50 all False. But in both sets some unexpected negates slip through. The only explanation I can think of right now is that the generator has generated a string, attributed to Gerrit that is more likely to have been written by Truus. This could be true due to the randimization in the generation.\n",
    "\n",
    "_Question_: Can you think of a better explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are defining the ``evaluate_cross_validation()`` function as we did in previous notebooks. As it seems that we are using this function in many notebooks, it would be a good idea to put it (together with other goodies) in a module. For now, we just repeat ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/requinard/.virtualenvs/datascience/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from numpy import mean\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # Create a k-fold cross validation iterator of k=5 folds\n",
    "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    # By default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print(scores)\n",
    "    print(\"Mean score: {0:.3f}\".format(mean(scores)))\n",
    "    print(\"Standard error of the mean: (+/-{0:.3f})\".format(sem(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we create our validation folds from our complete dataset as contrasted to creating folds from the training set in the SVM notebook ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soccer soccer car difficult difficult soccer difficult soccer car difficult car', 'soccer boring car boring contract boring difficult soccer car soccer contract boring car energy soccer']\n",
      "\n",
      "[[0 3 0 4 0 4 0]\n",
      " [4 3 2 1 1 4 0]]\n",
      "\n",
      "['boring', 'car', 'contract', 'difficult', 'energy', 'soccer', 'sweet']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "print(X[:2])\n",
    "\n",
    "print()\n",
    "\n",
    "print(vec.fit_transform(X).toarray()[:2])\n",
    "\n",
    "print()\n",
    "\n",
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's that vectorizer thing doing? Well, our Naive Bayes classifier (see below) can only deal with numeric data. So we have to map the texts to numeric data. That's in short what the vectorizer does: it creates a vector of features that we can give numeric values. The CountVectorizer is one of the simplest vectorizers available: it just creates a feature for each unique word in the text an then counts occurrences of each word in a text.\n",
    "\n",
    "There are also other vectorizers available in sklearn, such as the HashingVectorizer. Using the HashingVectorizer leads to a smaller feature space as different unique words may be hashed to the same bucket. The buckets form the feature space. Also, vectorizers may have paramaters. See the sklearn docs for which params you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95  0.8   0.95  0.95  1.  ]\n",
      "Mean score: 0.930\n",
      "Standard error of the mean: (+/-0.034)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "\n",
    "evaluate_cross_validation(clf, X, y, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's pretty accurate. Of course, that's especially because we've carefully crafted our dataset and took care to create users (Gerrit and Truus) that have really different writing styles. As an exercise you should create users with less distinct writing styles. You would probably see a lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car sweet contract sweet sweet car soccer sweet', 'soccer difficult sweet contract energy soccer difficult soccer car car boring contract sweet']\n",
      "['Gerrit', 'Gerrit']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    "print(X_train[:2])\n",
    "print(y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets fit our modelfrom the train set and test it against the test set. Explain why our test set contains 25 measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.96\n",
      "Accuracy on testing set:\n",
      "0.92\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Gerrit       0.80      1.00      0.89         8\n",
      "      Truus       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.94      0.92      0.92        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 8  0]\n",
      " [ 2 15]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already tested the sentence below in our Excel sheet and it calculated Truus as an author. Sure enough our classifier also predicts Truus as the original author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Truus'], \n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([\"contract energy contract sweet contract soccer contract energy difficult energy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to create a sentence generator for a small language (say, consisting of 10 words). You may get inspiration from the Excel generator, but of course you will use Python to create the generator. Generate a dataset (corpus) of 100 sentences attributed evenly to 2 authors. Your generator should take into account word preference of an author. Show that the more distinct preferences are, the more accurate your classifier is. And vice versa, the less distinct word preference is, the less accurate your classifier will be.\n",
    "\n",
    "This is quite a challenging assignment, but being almost halfway the course, we think you should be able to succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
